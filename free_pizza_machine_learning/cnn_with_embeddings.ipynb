{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5449ea86-ff90-45e0-a606-acba66cf853b",
   "metadata": {},
   "source": [
    "# CNN with Embeddings - Post Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a18349-ccf4-4ded-8acb-42c6231c18f9",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62f6a3e3-c615-4f78-80f8-806b92882c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jaredfeldman/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jaredfeldman/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# standard\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "# tf and keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# plots\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for stop words\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# for standardizing text\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a1e523-4819-4cb8-86b4-4a413fc8c788",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Initial Data Load and File Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14a5dfc6-23be-4461-a6c3-2cd693f8e517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>giver_username_if_known</th>\n",
       "      <th>number_of_downvotes_of_request_at_retrieval</th>\n",
       "      <th>number_of_upvotes_of_request_at_retrieval</th>\n",
       "      <th>post_was_edited</th>\n",
       "      <th>request_id</th>\n",
       "      <th>request_number_of_comments_at_retrieval</th>\n",
       "      <th>request_text</th>\n",
       "      <th>request_text_edit_aware</th>\n",
       "      <th>request_title</th>\n",
       "      <th>requester_account_age_in_days_at_request</th>\n",
       "      <th>...</th>\n",
       "      <th>requester_received_pizza</th>\n",
       "      <th>requester_subreddits_at_request</th>\n",
       "      <th>requester_upvotes_minus_downvotes_at_request</th>\n",
       "      <th>requester_upvotes_minus_downvotes_at_retrieval</th>\n",
       "      <th>requester_upvotes_plus_downvotes_at_request</th>\n",
       "      <th>requester_upvotes_plus_downvotes_at_retrieval</th>\n",
       "      <th>requester_user_flair</th>\n",
       "      <th>requester_username</th>\n",
       "      <th>unix_timestamp_of_request</th>\n",
       "      <th>unix_timestamp_of_request_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_l25d7</td>\n",
       "      <td>0</td>\n",
       "      <td>Hi I am in need of food for my 4 children we a...</td>\n",
       "      <td>Hi I am in need of food for my 4 children we a...</td>\n",
       "      <td>Request Colorado Springs Help Us Please</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>nickylvst</td>\n",
       "      <td>1317852607</td>\n",
       "      <td>1317849007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N/A</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_rcb83</td>\n",
       "      <td>0</td>\n",
       "      <td>I spent the last money I had on gas today. Im ...</td>\n",
       "      <td>I spent the last money I had on gas today. Im ...</td>\n",
       "      <td>[Request] California, No cash and I could use ...</td>\n",
       "      <td>501.1111</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>[AskReddit, Eve, IAmA, MontereyBay, RandomKind...</td>\n",
       "      <td>34</td>\n",
       "      <td>4258</td>\n",
       "      <td>116</td>\n",
       "      <td>11168</td>\n",
       "      <td>None</td>\n",
       "      <td>fohacidal</td>\n",
       "      <td>1332652424</td>\n",
       "      <td>1332648824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  giver_username_if_known  number_of_downvotes_of_request_at_retrieval  \\\n",
       "0                     N/A                                            0   \n",
       "1                     N/A                                            2   \n",
       "\n",
       "   number_of_upvotes_of_request_at_retrieval  post_was_edited request_id  \\\n",
       "0                                          1                0   t3_l25d7   \n",
       "1                                          5                0   t3_rcb83   \n",
       "\n",
       "   request_number_of_comments_at_retrieval  \\\n",
       "0                                        0   \n",
       "1                                        0   \n",
       "\n",
       "                                        request_text  \\\n",
       "0  Hi I am in need of food for my 4 children we a...   \n",
       "1  I spent the last money I had on gas today. Im ...   \n",
       "\n",
       "                             request_text_edit_aware  \\\n",
       "0  Hi I am in need of food for my 4 children we a...   \n",
       "1  I spent the last money I had on gas today. Im ...   \n",
       "\n",
       "                                       request_title  \\\n",
       "0            Request Colorado Springs Help Us Please   \n",
       "1  [Request] California, No cash and I could use ...   \n",
       "\n",
       "   requester_account_age_in_days_at_request  ...  requester_received_pizza  \\\n",
       "0                                    0.0000  ...                     False   \n",
       "1                                  501.1111  ...                     False   \n",
       "\n",
       "                     requester_subreddits_at_request  \\\n",
       "0                                                 []   \n",
       "1  [AskReddit, Eve, IAmA, MontereyBay, RandomKind...   \n",
       "\n",
       "   requester_upvotes_minus_downvotes_at_request  \\\n",
       "0                                             0   \n",
       "1                                            34   \n",
       "\n",
       "   requester_upvotes_minus_downvotes_at_retrieval  \\\n",
       "0                                               1   \n",
       "1                                            4258   \n",
       "\n",
       "   requester_upvotes_plus_downvotes_at_request  \\\n",
       "0                                            0   \n",
       "1                                          116   \n",
       "\n",
       "   requester_upvotes_plus_downvotes_at_retrieval  requester_user_flair  \\\n",
       "0                                              1                  None   \n",
       "1                                          11168                  None   \n",
       "\n",
       "   requester_username  unix_timestamp_of_request  \\\n",
       "0           nickylvst                 1317852607   \n",
       "1           fohacidal                 1332652424   \n",
       "\n",
       "   unix_timestamp_of_request_utc  \n",
       "0                     1317849007  \n",
       "1                     1332648824  \n",
       "\n",
       "[2 rows x 32 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "training_data = pd.read_json(\"train.json\")\n",
    "training_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4b33c95-7cda-4de7-a165-f47ac5595a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "request_text_edit_aware     object\n",
       "requester_received_pizza      bool\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_text = training_data[['request_text_edit_aware', 'requester_received_pizza']]\n",
    "training_data_text.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8479de64-22a3-43af-956b-407afebb1414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_text_edit_aware</th>\n",
       "      <th>requester_received_pizza</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi I am in need of food for my 4 children we a...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I spent the last money I had on gas today. Im ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My girlfriend decided it would be a good idea ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's cold, I'n hungry, and to be completely ho...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hey guys:\\n I love this sub. I think it's grea...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4035</th>\n",
       "      <td>Is anyone out there kind enough to help me out...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4036</th>\n",
       "      <td>If someone could hook me up with a $15 gift ca...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4037</th>\n",
       "      <td>Have today off, soo I'll be stuck in the house...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4038</th>\n",
       "      <td>I've never done anything like this before, but...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4039</th>\n",
       "      <td>Like the title says, had to pay an unexpected ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4040 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                request_text_edit_aware  \\\n",
       "0     Hi I am in need of food for my 4 children we a...   \n",
       "1     I spent the last money I had on gas today. Im ...   \n",
       "2     My girlfriend decided it would be a good idea ...   \n",
       "3     It's cold, I'n hungry, and to be completely ho...   \n",
       "4     hey guys:\\n I love this sub. I think it's grea...   \n",
       "...                                                 ...   \n",
       "4035  Is anyone out there kind enough to help me out...   \n",
       "4036  If someone could hook me up with a $15 gift ca...   \n",
       "4037  Have today off, soo I'll be stuck in the house...   \n",
       "4038  I've never done anything like this before, but...   \n",
       "4039  Like the title says, had to pay an unexpected ...   \n",
       "\n",
       "      requester_received_pizza  \n",
       "0                        False  \n",
       "1                        False  \n",
       "2                        False  \n",
       "3                        False  \n",
       "4                        False  \n",
       "...                        ...  \n",
       "4035                     False  \n",
       "4036                      True  \n",
       "4037                     False  \n",
       "4038                     False  \n",
       "4039                     False  \n",
       "\n",
       "[4040 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70db21df-65ff-458e-abb2-482291cf9075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "994\n",
      "3046\n",
      "4040\n"
     ]
    }
   ],
   "source": [
    "posts_pizza = training_data_text[training_data_text['requester_received_pizza'] == True]\n",
    "posts_no_pizza = training_data_text[training_data_text['requester_received_pizza'] == False]\n",
    "\n",
    "print(len(posts_pizza))\n",
    "print(len(posts_no_pizza))\n",
    "print(len(posts_pizza) + len(posts_no_pizza))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cdfacc8-5b46-4725-aff7-ad67778aef95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "994\n",
      "3046\n",
      "4040\n"
     ]
    }
   ],
   "source": [
    "posts_pizza_list = list(posts_pizza['request_text_edit_aware'])\n",
    "posts_no_pizza_list = list(posts_no_pizza['request_text_edit_aware'])\n",
    "\n",
    "print(len(posts_pizza_list))\n",
    "print(len(posts_no_pizza_list))\n",
    "print(len(posts_pizza_list) + len(posts_no_pizza_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35cc1497-fdbe-47fc-8153-3aa48d1e3701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# create files for each list\n",
    "# uncomment below to run but files already created and moved\n",
    "\n",
    "#for index, post in enumerate(posts_no_pizza_list):\n",
    "    #print(index)\n",
    "    #print(post)\n",
    "    #with io.open(\"file_\" + str(index) + \".txt\", 'w', encoding = 'utf-8') as f:\n",
    "        #f.write(post)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2298ad-3aab-4016-a714-cac11bcb81f3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "468135d6-dfee-47de-879b-9f8aefa7e25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    # Convert the input_data to lowercase\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    \n",
    "    # Remove any '<br />' tags from the text and replace them with a space\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    \n",
    "    # Remove any punctuation from the text\n",
    "    # 're.escape(string.punctuation)' escapes all punctuation characters for use in the regex pattern\n",
    "    # The regular expression pattern '[%s]' % re.escape(string.punctuation) matches any punctuation character\n",
    "    # and replaces it with an empty string, effectively removing it from the text.\n",
    "    # For example, if the input_data is \"Hello, world!\", the regex_replace will return \"Hello world\"    \n",
    "    return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation),\n",
    "                                  '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f704ac2-07c9-4d48-8b1e-d0551381727e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement a learning rate schedule to reduce learning rate each epoch\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    initial_learning_rate = 0.01 # initial learning rate\n",
    "    decay_steps = 5 # number of epochs to start decay\n",
    "    decay_rate = 0.5 # decay rate\n",
    "\n",
    "    # Compute the learning rate for the current epoch using exponential decay\n",
    "    # The formula used is: lr = initial_learning_rate * (decay_rate ** (epoch // decay_steps))\n",
    "    # The double division '//' ensures integer division so that only after 'decay_steps' epochs,\n",
    "    # the learning rate gets reduced.\n",
    "    lr = initial_learning_rate * (decay_rate ** (epoch // decay_steps))\n",
    "    \n",
    "    # Return the computed learning rate for the current epoch    \n",
    "    return lr\n",
    "\n",
    "# Create the LearningRateScheduler callback\n",
    "# The LearningRateScheduler callback will call the 'lr_schedule' function\n",
    "# at the beginning of each epoch to determine the learning rate for that epoch.\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3111f9f-3ff3-4ba4-beb8-d16f30a58e64",
   "metadata": {},
   "source": [
    "## Below are all of the different models\n",
    "\n",
    "It seems changing the input size does not improve accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181835d6-dc9f-4723-b8c6-15f4957f02f1",
   "metadata": {},
   "source": [
    "## Model 1: All post content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a78bd86a-4172-4a02-a538-e5cf56401f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4040 files belonging to 2 classes.\n",
      "Using 3636 files for training.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # divide dataset into batches of 32 samples each\n",
    "seed = 123 # for reproduceability\n",
    "\n",
    "# raw_train_ds will be a TensorFlow dataset that contains\n",
    "# batches of text data and their corresponding labels\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'posts/all_posts', # pull data from this directory\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.1, \n",
    "    subset='training', \n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c8c5ee1-c361-4700-954c-aae7826213d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post b\"Broke ass-college student here; got paid 10 days ago and all of my paycheck went to rent and bills. I've got no groceries in the house and all my roommates are back home for the summer. No gas in the car and the bike's out of commission :(. If any kind redditor could hook me up I'd be able to eat for two days :D\\n\\ngoogle maps link to domino's in my area\\nhttp://maps.google.com/maps/place?cid=10239146004443176581&amp;q=domino's+pizza,+chico,+ca&amp;hl=en&amp;sll=39.722827,-121.848776&amp;sspn=0.012468,0.024856&amp;ie=UTF8&amp;ll=39.733066,-121.864429&amp;spn=0,0&amp;z=15\\n\\n\\nEDIT: oh, and of course: I'll be pizzaing it forward when my paycheck comes next month. this is a ridiculously cool subreddit.\"\n",
      "Label 1\n",
      "Post b\"21/m/sfbay multi year lurker first time poster!\\nme! -&gt; http://imgur.com/VXtY6\\n\\nI'm just really hungry and I've got no money right now. Been unemployed for 6 months but just got a real great job offer at a consulting firm in berkeley. so i'd like to do something special to celebrate but i'm in the red right now.\\n\\nafter jan 2nd i will be employed with a well paying job and will definitely pay it forward.\\n\\nI'm stoked!\"\n",
      "Label 1\n",
      "Post b'would love a pizza. pm me if you wanna be kind :D'\n",
      "Label 0\n"
     ]
    }
   ],
   "source": [
    "# print some examples\n",
    "\n",
    "# Take one batch (batch_size=32) from the 'raw_train_ds' dataset\n",
    "for text_batch, label_batch in raw_train_ds.take(1):\n",
    "    # Loop over the first three samples in the batch\n",
    "    for i in range(3):\n",
    "        # Convert and print the text data and their corresponding labels for each sample\n",
    "        print(\"Post\", text_batch.numpy()[i])\n",
    "        print(\"Label\", label_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cbdd449-311a-4eef-bc62-9ee9c0e7e886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 corresponds to no_pizza\n",
      "Label 1 corresponds to pizza\n"
     ]
    }
   ],
   "source": [
    "# print what each label (0 or 1) corresponds to\n",
    "\n",
    "print(\"Label 0 corresponds to\", raw_train_ds.class_names[0])\n",
    "print(\"Label 1 corresponds to\", raw_train_ds.class_names[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03a3f6da-c65a-4887-86d8-6fb2e665f633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4040 files belonging to 2 classes.\n",
      "Using 404 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# Create a TensorFlow dataset for validation from text files stored in the directory\n",
    "# The 'text_dataset_from_directory' function automatically labels the text data based on subdirectories.\n",
    "# In this case, text files are stored in the 'posts/all_posts' directory, and subdirectories inside 'all_posts'\n",
    "# represent different classes or categories of text data.\n",
    "\n",
    "raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'posts/all_posts', # directory where .txt files are stored\n",
    "    batch_size=batch_size, # same as above\n",
    "    validation_split=0.1, # same as above\n",
    "    subset='validation', # this time, we're creating the validation set\n",
    "    seed=seed) # same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b660bdfe-fee4-4ad2-a4b6-aa0b7529999d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum number of unique tokens (words) to keep in the vocabulary\n",
    "max_features = 15000\n",
    "\n",
    "# Set the maximum sequence length of the tokenized text data\n",
    "sequence_length = 400\n",
    "\n",
    "# Create a TextVectorization layer for tokenizing and vectorizing text data\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization, # Preprocessing function for standardizing text data, defined earlier\n",
    "    max_tokens=max_features,# Maximum number of unique tokens to keep in the vocabulary\n",
    "    output_mode='int', # Output mode as integer indices (integers represent tokens)\n",
    "    output_sequence_length=sequence_length) # Maximum sequence length of the tokenized text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17c9ed89-84d4-458a-9711-0e8dbca7a38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "    # Expand the dimensions of the 'text' tensor to make it compatible with the 'vectorize_layer'\n",
    "    # The '-1' argument adds a new axis at the end, effectively converting the 1D tensor 'text' into a 2D tensor\n",
    "    # For example, if 'text' was [word1, word2, word3], it will become [[word1], [word2], [word3]]\n",
    "    text = tf.expand_dims(text, -1)\n",
    "\n",
    "    # Pass the expanded 'text' tensor through the 'vectorize_layer' to convert it into numerical sequences\n",
    "    # The 'vectorize_layer' was defined earlier and tokenizes the text data into integer sequences.\n",
    "    # It also applies the 'custom_standardization' function for preprocessing the text.\n",
    "    vectorized_text = vectorize_layer(text)\n",
    "\n",
    "    # Return the vectorized_text and its corresponding 'label'.\n",
    "    # 'label' is associated with the 'text' and represents the class/category of the text.\n",
    "    return vectorized_text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0dc6434b-9c2a-4bdc-97b3-b821b238cd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataset 'train_text' that contains only the text data (x) from 'raw_train_ds'\n",
    "# The 'train_text' dataset is created using the 'map' function,\n",
    "# which extracts only the 'x' (text) part of the input tuple (x, y).\n",
    "train_text = raw_train_ds.map(lambda x, y: x)\n",
    "\n",
    "# Adapt the 'vectorize_layer' to the training data\n",
    "# This step is necessary to build the vocabulary and tokenize the text data based on the training dataset.\n",
    "vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4c64520-95ac-419e-bb58-0c081eed2731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post---> tf.Tensor(b\"I'm helping my friend out who got kicked out of his house, letting him crash here and offering my time and whatever else he needs. As a result of helping him drink his way out of this (not healthy, I know, but we all deal in different ways, and he IS looking for a new place) we're broke and I'm out of ideas to cheer him up. Then I saw this place. It would be a perfect story, and cheer him up immensely, if some random stranger on the internet helped us out. \", shape=(), dtype=string)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Label---> pizza\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Vectorized post---> (<tf.Tensor: shape=(1, 400), dtype=int64, numpy=\n",
      "array([[  17,  430,    7,  191,   24,  121,   77,  949,   24,    8,  140,\n",
      "         139, 1326,  151, 2520,   62,    4, 1376,    7,   69,    4,  401,\n",
      "         245,   84,  624,   38,    5, 1852,    8,  430,  151, 1278,  140,\n",
      "         156,   24,    8,   20,   35, 1771,    2,   85,   18,   27,   40,\n",
      "         643,   10,  641, 1820,    4,   84,   16,  195,    9,    5,  106,\n",
      "         237,  100,   87,    4,   17,   24,    8, 1533,    3,  538,  151,\n",
      "          47,  165,    2,  878,   20,  237,   13,   15,   19,    5, 1442,\n",
      "         185,    4,  538,  151,   47, 2461,   26,   33,  268,  962,   21,\n",
      "           6,  534,  718,   81,   24,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0]])>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n"
     ]
    }
   ],
   "source": [
    "# Get the next batch of data from 'raw_train_ds'\n",
    "post_batch, label_batch = next(iter(raw_train_ds))\n",
    "\n",
    "# Extract the first post (text data) and its corresponding label from the batch\n",
    "first_post, first_label = post_batch[0], label_batch[0]\n",
    "\n",
    "# Print the raw text post\n",
    "print(\"Post--->\", first_post)\n",
    "\n",
    "# Print break\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Print the label of corresponding post\n",
    "print(\"Label--->\", raw_train_ds.class_names[first_label])\n",
    "\n",
    "# Print break\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Print the vectorized post and corresponding label\n",
    "print(\"Vectorized post--->\", vectorize_text(first_post, first_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2a7d059-844f-4d3a-ad28-fcbe4063517a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 12570\n",
      "958 --->  training\n",
      "135 --->  them\n"
     ]
    }
   ],
   "source": [
    "# Print the size of the entire vocabulary\n",
    "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))\n",
    "\n",
    "# Print two samples, from index 958 and index 135\n",
    "print(\"958 ---> \",vectorize_layer.get_vocabulary()[958])\n",
    "print(\"135 ---> \",vectorize_layer.get_vocabulary()[135])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b41acf5b-2c56-466e-8a8d-cd9b759cf653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the 'vectorize_text' function to 'raw_train_ds' to convert its text data into vectorized numerical sequences\n",
    "# The 'vectorize_text' function preprocesses and tokenizes the text data using the 'vectorize_layer'\n",
    "# The resulting dataset 'train_ds' contains batches of vectorized text data and their corresponding labels.\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "\n",
    "# Map the 'vectorize_text' function to 'raw_val_ds' to convert its text data into vectorized numerical sequences\n",
    "# The 'vectorize_text' function preprocesses and tokenizes the text data using the 'vectorize_layer'\n",
    "# The resulting dataset 'val_ds' contains batches of vectorized text data and their corresponding labels.\n",
    "val_ds = raw_val_ds.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "603c98d0-8bac-4600-9aa7-711c64316c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AUTOTUNE, which allows TensorFlow to automatically tune the buffer size for optimal performance.\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Cache the 'train_ds' dataset to improve data loading speed during training.\n",
    "# Caching stores the data in memory after the first iteration through the dataset,\n",
    "# so subsequent iterations can access it faster without re-reading from the disk.\n",
    "train_ds = train_ds.cache()\n",
    "\n",
    "# Prefetch the 'train_ds' dataset to overlap data preprocessing and model execution.\n",
    "# Prefetching allows the data pipeline to fetch data for the next batch while the current batch is being processed,\n",
    "# reducing the idle time and maximizing GPU utilization during training.\n",
    "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Cache the 'val_ds' dataset to improve data loading speed during validation.\n",
    "# Caching stores the data in memory after the first iteration through the dataset,\n",
    "# so subsequent iterations can access it faster without re-reading from the disk.\n",
    "val_ds = val_ds.cache()\n",
    "\n",
    "# Prefetch the 'val_ds' dataset to overlap data preprocessing and model execution during validation.\n",
    "# Prefetching allows the data pipeline to fetch data for the next batch while the current batch is being processed,\n",
    "# reducing the idle time and maximizing GPU utilization during validation.\n",
    "val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3de048d5-d9d6-4c65-ab74-408c57bdf94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    # Clear previous session and set a random seed for reproducibility\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(0)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # Add an Embedding layer to convert text data into dense numerical vectors\n",
    "    model.add(tf.keras.layers.Embedding(\n",
    "        input_dim = max_features,  # size of feature vocabulary (number of unique words), defined earlier\n",
    "        output_dim = 16,  # dimension of the embedding vector for each word (embedding dimension)\n",
    "        input_length=sequence_length  # Length of each input sequence (number of words in a post), defined earlier\n",
    "    ))\n",
    "    \n",
    "    # Add a 1D Convolutional layer to capture local patterns in the text\n",
    "    model.add(tf.keras.layers.Conv1D(\n",
    "        filters=16,        # Number of filters (output channels)\n",
    "        strides=3,         # Stride size for the convolution operation\n",
    "        padding='same',    # Padding to ensure the output has the same length as the input\n",
    "        kernel_size=16,    # Length of the convolutional kernel (window size)\n",
    "        activation='relu'\n",
    "    ))\n",
    "\n",
    "    # Add a Global Average Pooling layer to aggregate information over the sequence dimension\n",
    "    # This reduces the sequence length to 1, making each post represented by a single vector\n",
    "    model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "\n",
    "    # Add an additional Dense layer with 16 units and ReLU activation\n",
    "    model.add(tf.keras.layers.Dense(\n",
    "      units=32,        \n",
    "      activation='relu'))\n",
    "    \n",
    "    # Add the output layer with 1 unit and sigmoid activation for binary classification    \n",
    "    model.add(tf.keras.layers.Dense(\n",
    "      units=1,        \n",
    "      activation='sigmoid'))\n",
    "\n",
    "    # Compile the model with binary cross-entropy loss for binary classification\n",
    "    # Use the Adam optimizer with default learning rate\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                optimizer=tf.keras.optimizers.legacy.Adam(learning_rate = 0.001), #comment out to use learning rate schedule\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53d14596-a952-4ece-b28f-62ff90b0aca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.src.layers.core.embedding.Embedding at 0x111993bb0>,\n",
       " <keras.src.layers.convolutional.conv1d.Conv1D at 0x2cf4008e0>,\n",
       " <keras.src.layers.pooling.global_average_pooling1d.GlobalAveragePooling1D at 0x2a9c596d0>,\n",
       " <keras.src.layers.core.dense.Dense at 0x2a9c59f70>,\n",
       " <keras.src.layers.core.dense.Dense at 0x2cf1f5550>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 400, 16)           240000    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 134, 16)           4112      \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 16)                0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                544       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 244689 (955.82 KB)\n",
      "Trainable params: 244689 (955.82 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the model using the build_model function\n",
    "model_all_posts = build_model()\n",
    "\n",
    "# Display the model layers.\n",
    "display(model_all_posts.layers)\n",
    "\n",
    "# Display a summary of the mdel architecture\n",
    "display(model_all_posts.summary())\n",
    "\n",
    "# Retrieve the embeddings layer weights\n",
    "# The embeddings are stored in the last layer of the model\n",
    "embeddings = model_all_posts.layers[-1].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2de88199-f2c1-4f21-8261-bf2f1b8581db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "114/114 [==============================] - 8s 67ms/step - loss: 0.5783 - accuracy: 0.7456 - val_loss: 0.5505 - val_accuracy: 0.7574\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 6s 56ms/step - loss: 0.5506 - accuracy: 0.7536 - val_loss: 0.5500 - val_accuracy: 0.7574\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 6s 56ms/step - loss: 0.5387 - accuracy: 0.7547 - val_loss: 0.5520 - val_accuracy: 0.7500\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 6s 55ms/step - loss: 0.4904 - accuracy: 0.7800 - val_loss: 0.5659 - val_accuracy: 0.7228\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 6s 56ms/step - loss: 0.3998 - accuracy: 0.8432 - val_loss: 0.5781 - val_accuracy: 0.7129\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 6s 55ms/step - loss: 0.2767 - accuracy: 0.8999 - val_loss: 0.6043 - val_accuracy: 0.7302\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 6s 55ms/step - loss: 0.1803 - accuracy: 0.9403 - val_loss: 0.6900 - val_accuracy: 0.7277\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 6s 55ms/step - loss: 0.1176 - accuracy: 0.9642 - val_loss: 0.8148 - val_accuracy: 0.7450\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 6s 55ms/step - loss: 0.0815 - accuracy: 0.9791 - val_loss: 0.9117 - val_accuracy: 0.7302\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 6s 54ms/step - loss: 0.0611 - accuracy: 0.9846 - val_loss: 1.0442 - val_accuracy: 0.7376\n"
     ]
    }
   ],
   "source": [
    "history = model_all_posts.fit(\n",
    "    train_ds,\n",
    "    batch_size = 16,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10,\n",
    "    validation_split=0.1)#,\n",
    "    #callbacks=[lr_scheduler]) # with this commented out, we'll use the default learning rate of 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2357d4b3-78bd-4cd8-81db-af4b5938ae72",
   "metadata": {},
   "source": [
    "### Learning rate = 0.001, 15 epochs\n",
    "\n",
    "|max_features|sequence_length|embedding_output_dim|conv1d_filters|conv1d_strides|conv1d_kernelsize|dense_units|best_epoch|training_accuracy|val_accuracy|\n",
    "|------------|---------------|--------------------|--------------|--------------|-----------------|-----------|----------|-----------------|------------|\n",
    "|15000       |400            |16                  |16            |3             |12               |16         |12        |0.9873           |0.7277      |\n",
    "|<font color=\"red\">10000</font>|400 |16           |16            |3             |12               |16         |11        |0.9854           |0.7277      |\n",
    "|15000       |<font color=\"red\">300</font>| 16    |16            |3             |12               |16         |9         |0.9816           |0.7351      |\n",
    "|15000       |<font color=\"red\">500</font>| 16    |16            |3             |12               |16         |8         |0.9323           |0.7302      |\n",
    "|15000       |400| <font color=\"red\">8</font>     |16            |3             |12               |16         |8         |0.9156           |0.7252      |\n",
    "|15000       |400| <font color=\"red\">32</font>    |16            |3             |12               |16         |5         |0.8685           |0.7450      |\n",
    "|15000       |400            |16                  |<font color=\"red\">8</font> |3 |12              |16         |11        |0.8102           |0.7153      |\n",
    "|15000       |400            |16                  |<font color=\"red\">32</font> |3 |12             |16         |6         |0.8793           |0.7302      |\n",
    "|15000       |400            |16                  |16            |<font color=\"red\">2</font> |12  |16         |15        |0.9890           |0.7228      |\n",
    "|15000       |400            |16                  |16            |<font color=\"red\">1</font> |12  |16         |5         |0.8034           |0.7327      |\n",
    "|15000       |400            |16                  |16            |3 |<font color=\"red\">8</font>   |16         |7         |0.8377           |0.7351      |\n",
    "|15000       |400            |16                  |16            |3 |<font color=\"red\">16</font>  |16         |10        |0.9769           |0.7376      |\n",
    "|15000       |400            |16                  |16            |3 |12 |<font color=\"red\">8</font>           |9         |0.9150           |0.7351      |\n",
    "|15000       |400            |16                  |16            |3 |12 |<font color=\"red\">32</font>          |9         |0.9714           |0.7475      |\n",
    "|15000       |400            |16                  |16            |3 |12 |<font color=\"red\">64</font>          |10        |0.9805           |0.7252      |\n",
    "|15000       |400            |<font color=\"red\">32</font> |16    |3 |12 |<font color=\"red\">32</font>          |7         |0.9334           |0.7252      |\n",
    "\n",
    "\n",
    "`best_epoch` defined as the one having the best val_accuracy with training_accuracy >80%\n",
    "\n",
    "##### After 100 epochs with learning rate of 0.0001:\n",
    "Epoch 100/100\n",
    "114/114 [==============================] - 7s 63ms/step - loss: 0.0271 - accuracy: 0.9928 - val_loss: 1.4123 - val_accuracy: 0.7178"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2518d3e4-569d-418c-be80-721b5c20dc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "     0  1\n",
      "0  306  0\n",
      "1   98  0\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the validation dataset\n",
    "y_true = []  # Ground truth (actual labels)\n",
    "y_pred = []  # Predicted labels\n",
    "\n",
    "for x_val, y_val in val_ds:\n",
    "    predictions = model_all_posts.predict(x_val)\n",
    "    y_true.extend(y_val)  # Assuming y_val contains integer labels\n",
    "    y_pred.extend(np.argmax(predictions, axis=1))\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "confusion_matrix_df = pd.DataFrame(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(confusion_matrix_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "596e298e-39ba-43bd-b8b9-0635a35ad558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[306,   0],\n",
       "       [ 98,   0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0e452d-82a2-4211-acde-6bfaf091085c",
   "metadata": {},
   "source": [
    "## Model 2: 700 Samples of Each Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c056f69-1fcd-4ce8-8e4b-ff7934a56bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1401 files belonging to 2 classes.\n",
      "Using 1261 files for training.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # divide dataset into batches of 32 samples each\n",
    "seed = 123 # for reproduceability\n",
    "\n",
    "# raw_train_ds will be a TensorFlow dataset that contains\n",
    "# batches of text data and their corresponding labels\n",
    "raw_train_ds_700 = tf.keras.utils.text_dataset_from_directory( #only use sample of 700 posts for each classification\n",
    "    'posts/posts_700', # pull data from this directory\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.1, \n",
    "    subset='training', \n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9fea78b5-c423-4359-93cc-bcd4f2146d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 corresponds to no_pizza_700\n",
      "Label 1 corresponds to pizza_700\n"
     ]
    }
   ],
   "source": [
    "#  print what each label (0 or 1) corresponds to\n",
    "\n",
    "print(\"Label 0 corresponds to\", raw_train_ds_700.class_names[0])\n",
    "print(\"Label 1 corresponds to\", raw_train_ds_700.class_names[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8748c0fa-d2e4-4127-9e39-d13df8920f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1401 files belonging to 2 classes.\n",
      "Using 140 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# Create a TensorFlow dataset for validation from text files stored in the directory\n",
    "# The 'text_dataset_from_directory' function automatically labels the text data based on subdirectories.\n",
    "# In this case, text files are stored in the 'posts/all_posts' directory, and subdirectories inside 'all_posts'\n",
    "# represent different classes or categories of text data.\n",
    "\n",
    "raw_val_ds_700 = tf.keras.utils.text_dataset_from_directory(\n",
    "    'posts/posts_700', # directory where .txt files are stored\n",
    "    batch_size=batch_size, # same as above\n",
    "    validation_split=0.1, # same as above\n",
    "    subset='validation', # this time, we're creating the validation set\n",
    "    seed=seed) # same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b0aa2fc-ea58-463e-a1ce-5fd26040c696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum number of unique tokens (words) to keep in the vocabulary\n",
    "max_features_700 = 15000\n",
    "\n",
    "# Set the maximum sequence length of the tokenized text data\n",
    "sequence_length_700 = 400\n",
    "\n",
    "# Create a TextVectorization layer for tokenizing and vectorizing text data\n",
    "vectorize_layer_700 = layers.TextVectorization(\n",
    "    standardize=custom_standardization, # Preprocessing function for standardizing text data, defined earlier\n",
    "    max_tokens=max_features_700,# Maximum number of unique tokens to keep in the vocabulary\n",
    "    output_mode='int', # Output mode as integer indices (integers represent tokens)\n",
    "    output_sequence_length=sequence_length_700) # Maximum sequence length of the tokenized text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67ba2d41-19b8-43fe-8503-e206f2a3e56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text_700(text, label):\n",
    "    # Expand the dimensions of the 'text' tensor to make it compatible with the 'vectorize_layer'\n",
    "    # The '-1' argument adds a new axis at the end, effectively converting the 1D tensor 'text' into a 2D tensor\n",
    "    # For example, if 'text' was [word1, word2, word3], it will become [[word1], [word2], [word3]]\n",
    "    text = tf.expand_dims(text, -1)\n",
    "\n",
    "    # Pass the expanded 'text' tensor through the 'vectorize_layer' to convert it into numerical sequences\n",
    "    # The 'vectorize_layer' was defined earlier and tokenizes the text data into integer sequences.\n",
    "    # It also applies the 'custom_standardization' function for preprocessing the text.\n",
    "    vectorized_text_700 = vectorize_layer_700(text)\n",
    "\n",
    "    # Return the vectorized_text and its corresponding 'label'.\n",
    "    # 'label' is associated with the 'text' and represents the class/category of the text.\n",
    "    return vectorized_text_700, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e55d301-9402-400e-8321-4eb7f36b4355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataset 'train_text' that contains only the text data (x) from 'raw_train_ds'\n",
    "# The 'train_text' dataset is created using the 'map' function,\n",
    "# which extracts only the 'x' (text) part of the input tuple (x, y).\n",
    "train_text_700 = raw_train_ds_700.map(lambda x, y: x)\n",
    "\n",
    "# Adapt the 'vectorize_layer' to the training data\n",
    "# This step is necessary to build the vocabulary and tokenize the text data based on the training dataset.\n",
    "vectorize_layer_700.adapt(train_text_700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8be5cae0-cdcc-44e5-a5a7-a51a04b0a305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the 'vectorize_text' function to 'raw_train_ds' to convert its text data into vectorized numerical sequences\n",
    "# The 'vectorize_text' function preprocesses and tokenizes the text data using the 'vectorize_layer'\n",
    "# The resulting dataset 'train_ds' contains batches of vectorized text data and their corresponding labels.\n",
    "train_ds_700 = raw_train_ds_700.map(vectorize_text_700)\n",
    "\n",
    "# Map the 'vectorize_text' function to 'raw_val_ds' to convert its text data into vectorized numerical sequences\n",
    "# The 'vectorize_text' function preprocesses and tokenizes the text data using the 'vectorize_layer'\n",
    "# The resulting dataset 'val_ds' contains batches of vectorized text data and their corresponding labels.\n",
    "val_ds_700 = raw_val_ds_700.map(vectorize_text_700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a807b91a-beba-432f-bca2-ab4647a72d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AUTOTUNE, which allows TensorFlow to automatically tune the buffer size for optimal performance.\n",
    "AUTOTUNE_700 = tf.data.AUTOTUNE\n",
    "\n",
    "# Cache the 'train_ds' dataset to improve data loading speed during training.\n",
    "# Caching stores the data in memory after the first iteration through the dataset,\n",
    "# so subsequent iterations can access it faster without re-reading from the disk.\n",
    "train_ds_700 = train_ds_700.cache()\n",
    "\n",
    "# Prefetch the 'train_ds' dataset to overlap data preprocessing and model execution.\n",
    "# Prefetching allows the data pipeline to fetch data for the next batch while the current batch is being processed,\n",
    "# reducing the idle time and maximizing GPU utilization during training.\n",
    "train_ds_700 = train_ds_700.prefetch(buffer_size=AUTOTUNE_700)\n",
    "\n",
    "# Cache the 'val_ds' dataset to improve data loading speed during validation.\n",
    "# Caching stores the data in memory after the first iteration through the dataset,\n",
    "# so subsequent iterations can access it faster without re-reading from the disk.\n",
    "val_ds_700 = val_ds_700.cache()\n",
    "\n",
    "# Prefetch the 'val_ds' dataset to overlap data preprocessing and model execution during validation.\n",
    "# Prefetching allows the data pipeline to fetch data for the next batch while the current batch is being processed,\n",
    "# reducing the idle time and maximizing GPU utilization during validation.\n",
    "val_ds_700 = val_ds_700.prefetch(buffer_size=AUTOTUNE_700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed39be90-d57c-48c1-bbb6-696f5d4e60b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.src.layers.core.embedding.Embedding at 0x2cea7d0d0>,\n",
       " <keras.src.layers.convolutional.conv1d.Conv1D at 0x2cf42bd90>,\n",
       " <keras.src.layers.pooling.global_average_pooling1d.GlobalAveragePooling1D at 0x2a9e167c0>,\n",
       " <keras.src.layers.core.dense.Dense at 0x2a9c59ca0>,\n",
       " <keras.src.layers.core.dense.Dense at 0x2a4281dc0>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 400, 16)           240000    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 134, 16)           4112      \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 16)                0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                544       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 244689 (955.82 KB)\n",
      "Trainable params: 244689 (955.82 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the model using the build_model function\n",
    "model_700 = build_model()\n",
    "\n",
    "# Display the model layers.\n",
    "display(model_700.layers)\n",
    "\n",
    "# Display a summary of the mdel architecture\n",
    "display(model_700.summary())\n",
    "\n",
    "# Retrieve the embeddings layer weights\n",
    "# The embeddings are stored in the last layer of the model\n",
    "embeddings = model_700.layers[-1].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb13f71b-4133-440f-be77-be5d89f98b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "40/40 [==============================] - 3s 74ms/step - loss: 0.6931 - accuracy: 0.5242 - val_loss: 0.6927 - val_accuracy: 0.6357\n",
      "Epoch 2/10\n",
      "40/40 [==============================] - 2s 54ms/step - loss: 0.6909 - accuracy: 0.5155 - val_loss: 0.6911 - val_accuracy: 0.5357\n",
      "Epoch 3/10\n",
      "40/40 [==============================] - 2s 55ms/step - loss: 0.6800 - accuracy: 0.5741 - val_loss: 0.6896 - val_accuracy: 0.6214\n",
      "Epoch 4/10\n",
      "40/40 [==============================] - 2s 55ms/step - loss: 0.6445 - accuracy: 0.6661 - val_loss: 0.6998 - val_accuracy: 0.6286\n",
      "Epoch 5/10\n",
      "40/40 [==============================] - 2s 55ms/step - loss: 0.5697 - accuracy: 0.7581 - val_loss: 0.7449 - val_accuracy: 0.6214\n",
      "Epoch 6/10\n",
      "40/40 [==============================] - 2s 54ms/step - loss: 0.4624 - accuracy: 0.8343 - val_loss: 0.8262 - val_accuracy: 0.6143\n",
      "Epoch 7/10\n",
      "40/40 [==============================] - 2s 54ms/step - loss: 0.3481 - accuracy: 0.8921 - val_loss: 0.9448 - val_accuracy: 0.5929\n",
      "Epoch 8/10\n",
      "40/40 [==============================] - 2s 55ms/step - loss: 0.2553 - accuracy: 0.9358 - val_loss: 1.0737 - val_accuracy: 0.5714\n",
      "Epoch 9/10\n",
      "40/40 [==============================] - 2s 54ms/step - loss: 0.1867 - accuracy: 0.9548 - val_loss: 1.2158 - val_accuracy: 0.5643\n",
      "Epoch 10/10\n",
      "40/40 [==============================] - 2s 54ms/step - loss: 0.1440 - accuracy: 0.9691 - val_loss: 1.3499 - val_accuracy: 0.5571\n"
     ]
    }
   ],
   "source": [
    "history_700 = model_700.fit(\n",
    "    train_ds_700,\n",
    "    batch_size = 16,\n",
    "    validation_data=val_ds_700,\n",
    "    epochs=10,\n",
    "    validation_split=0.1)#,\n",
    "    #callbacks=[lr_scheduler]) # with this commented out, we'll use the default learning rate of 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b0dfa6de-40dd-41fc-af24-225a75754bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "    0  1\n",
      "0  71  0\n",
      "1  69  0\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the validation dataset\n",
    "y_true_700 = []  # Ground truth (actual labels)\n",
    "y_pred_700 = []  # Predicted labels\n",
    "\n",
    "for x_val_700, y_val_700 in val_ds_700:\n",
    "    predictions_700 = model_700.predict(x_val_700)\n",
    "    y_true_700.extend(y_val_700)  # Assuming y_val contains integer labels\n",
    "    y_pred_700.extend(np.argmax(predictions_700, axis=1))\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "confusion_matrix_df_700 = pd.DataFrame(confusion_matrix(y_true_700, y_pred_700))\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(confusion_matrix_df_700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "14020157-d16d-45c8-b872-413fbcdd9194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[71,  0],\n",
       "       [69,  0]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_true_700, y_pred_700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "330b5bbc-bbb4-4c61-b52a-ba3b79309da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Failure       0.51      1.00      0.67        71\n",
      "     Success       0.00      0.00      0.00        69\n",
      "\n",
      "    accuracy                           0.51       140\n",
      "   macro avg       0.25      0.50      0.34       140\n",
      "weighted avg       0.26      0.51      0.34       140\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaredfeldman/miniforge3/envs/w207_final/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jaredfeldman/miniforge3/envs/w207_final/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jaredfeldman/miniforge3/envs/w207_final/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names = ['Failure', 'Success']\n",
    "print(classification_report(y_true_700, y_pred_700, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44edc670-029c-45fe-a408-a86e13b9f7d9",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f3dfac-15d5-4c97-b760-efacc374c584",
   "metadata": {},
   "source": [
    "### Format Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b2230ded-913c-4a07-a82d-40a2d6d03210",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_json(\"test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "76091f04-51fa-42d3-af4b-da4810ba6f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "giver_username_if_known                                object\n",
       "request_id                                             object\n",
       "request_text_edit_aware                                object\n",
       "request_title                                          object\n",
       "requester_account_age_in_days_at_request              float64\n",
       "requester_days_since_first_post_on_raop_at_request    float64\n",
       "requester_number_of_comments_at_request                 int64\n",
       "requester_number_of_comments_in_raop_at_request         int64\n",
       "requester_number_of_posts_at_request                    int64\n",
       "requester_number_of_posts_on_raop_at_request            int64\n",
       "requester_number_of_subreddits_at_request               int64\n",
       "requester_subreddits_at_request                        object\n",
       "requester_upvotes_minus_downvotes_at_request            int64\n",
       "requester_upvotes_plus_downvotes_at_request             int64\n",
       "requester_username                                     object\n",
       "unix_timestamp_of_request                               int64\n",
       "unix_timestamp_of_request_utc                           int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3effce44-f23c-4776-91e4-78ea2406726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_posts = test_data['request_text_edit_aware'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2bb280c5-c6e6-4c59-a86f-92ff1bd1c8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_posts_list = test_data_posts.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "de069408-3723-44c7-a5db-988604a93881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(test_data_posts))\n",
    "print(type(test_data_posts_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cef6a4df-6504-4ad2-8491-b93edce69dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(test_data_posts_list)\n",
    "sequences = tokenizer.texts_to_sequences(test_data_posts_list)\n",
    "type(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "feb5aba2-2700-4be2-8122-a162885fde2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1631, 400)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_length = 400\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "padded_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "80a73b76-c17d-4e80-9989-f990fa6d5202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TensorSliceDataset element_spec=(TensorSpec(shape=(400,), dtype=tf.int32, name=None),)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices((padded_sequences,))\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "caa928ec-676f-4249-ad2c-4bf9e94beac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefetch_buffer_size = tf.data.experimental.AUTOTUNE\n",
    "test_dataset = test_dataset.prefetch(buffer_size=prefetch_buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7d713918-4c11-4887-afa1-3a7a46b0b328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.prefetch_op._PrefetchDataset"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0cb60a80-004a-42d5-96a9-ad90cee1bbb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=(TensorSpec(shape=(None, 400), dtype=tf.int32, name=None),)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "03a58ad9-e905-415b-a0dd-fd2d01d1118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_submission_csv(predictions):\n",
    "    test_ids = test_data['request_id']\n",
    "    \n",
    "    d = {'request_id': test_ids,\n",
    "    'requester_received_pizza': predictions}\n",
    "    \n",
    "    df = pd.DataFrame(data=d)\n",
    "    df['requester_received_pizza'] = df['requester_received_pizza'].astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41768a6-294a-4a42-a69e-64ac00249126",
   "metadata": {},
   "source": [
    "### Model for all Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7cf7fd59-8a9e-4e68-8897-4c129ed93c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 19 calls to <function Model.make_predict_function.<locals>.predict_function at 0x2a9dc4d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "51/51 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "model_all_posts_predictions = model_all_posts.predict(\n",
    "    test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9613e614-5b9d-4c5e-9851-9ae4f971eb84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.4088858e-02],\n",
       "       [4.2589576e-04],\n",
       "       [6.9383256e-02],\n",
       "       ...,\n",
       "       [1.0342760e-02],\n",
       "       [1.9164889e-05],\n",
       "       [8.1833206e-02]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_all_posts_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9ad5ea42-2b33-4734-bc6e-510a4a05d1ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.4088858e-02, 4.2589576e-04, 6.9383256e-02, ..., 1.0342760e-02,\n",
       "       1.9164889e-05, 8.1833206e-02], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_all_posts_predictions_flat = model_all_posts_predictions.flatten()\n",
    "model_all_posts_predictions_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8f023e8c-799a-4dc4-b758-0b7a407d40ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_id</th>\n",
       "      <th>requester_received_pizza</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_i8iy4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_1mfqi0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_lclka</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_1jdgdj</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_t2qt4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626</th>\n",
       "      <td>t3_knttk</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1627</th>\n",
       "      <td>t3_11wza2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1628</th>\n",
       "      <td>t3_iwbsf</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1629</th>\n",
       "      <td>t3_nys7g</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1630</th>\n",
       "      <td>t3_17pmtu</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1631 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     request_id  requester_received_pizza\n",
       "0      t3_i8iy4                         0\n",
       "1     t3_1mfqi0                         0\n",
       "2      t3_lclka                         0\n",
       "3     t3_1jdgdj                         0\n",
       "4      t3_t2qt4                         0\n",
       "...         ...                       ...\n",
       "1626   t3_knttk                         0\n",
       "1627  t3_11wza2                         0\n",
       "1628   t3_iwbsf                         0\n",
       "1629   t3_nys7g                         0\n",
       "1630  t3_17pmtu                         0\n",
       "\n",
       "[1631 rows x 2 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_posts = get_submission_csv(model_all_posts_predictions_flat)\n",
    "\n",
    "df_all_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f9afb4d4-4abd-409d-be52-313a1fe3ef84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all_posts.to_csv('cnn_model_all_posts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6452ee6a-6a9c-4209-9e60-d18344be741f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "requester_received_pizza\n",
       "0    1631\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_posts['requester_received_pizza'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1363e970-aa1f-40b7-9bd5-3729c1b83b74",
   "metadata": {},
   "source": [
    "### Model for 700 Posts of Each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d377610f-1169-400c-b3df-47ac95236324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "model_700_posts_predictions = model_700.predict(\n",
    "    test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "754b59cb-697f-4c8d-a8a7-d9da5bd7f9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6087672 , 0.9183351 , 0.9220668 , ..., 0.08755773, 0.9997502 ,\n",
       "       0.10727199], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_700_posts_predictions_flat = model_700_posts_predictions.flatten()\n",
    "model_700_posts_predictions_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "906322c7-477e-47bd-b84c-417686af3ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_id</th>\n",
       "      <th>requester_received_pizza</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_i8iy4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_1mfqi0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_lclka</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_1jdgdj</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_t2qt4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626</th>\n",
       "      <td>t3_knttk</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1627</th>\n",
       "      <td>t3_11wza2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1628</th>\n",
       "      <td>t3_iwbsf</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1629</th>\n",
       "      <td>t3_nys7g</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1630</th>\n",
       "      <td>t3_17pmtu</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1631 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     request_id  requester_received_pizza\n",
       "0      t3_i8iy4                         0\n",
       "1     t3_1mfqi0                         0\n",
       "2      t3_lclka                         0\n",
       "3     t3_1jdgdj                         0\n",
       "4      t3_t2qt4                         0\n",
       "...         ...                       ...\n",
       "1626   t3_knttk                         0\n",
       "1627  t3_11wza2                         0\n",
       "1628   t3_iwbsf                         0\n",
       "1629   t3_nys7g                         0\n",
       "1630  t3_17pmtu                         0\n",
       "\n",
       "[1631 rows x 2 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_700_posts = get_submission_csv(model_700_posts_predictions_flat)\n",
    "\n",
    "df_700_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "795de93c-7dd3-4a8d-8957-08cc7e79722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_700_posts.to_csv('cnn_model_700_posts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "15316152-c504-4cb8-a296-998d9d2e2900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "requester_received_pizza\n",
       "0    1627\n",
       "1       4\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_700_posts['requester_received_pizza'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
